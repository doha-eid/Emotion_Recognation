{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "df = pd.read_csv('ECG.csv')\n",
    "X = df.drop(['target','Unnamed: 0'] , axis=1)\n",
    "y = df.target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decisionTree(x_train, y_train, x_test, y_test, first_class_index = 0):\n",
    "    classifiers = []\n",
    "    for i in range(0, 9):\n",
    "        clf = DecisionTreeClassifier()\n",
    "        y_train_binary = (y_train == i)\n",
    "        clf.fit(x_train, y_train_binary)\n",
    "        classifiers.append(clf)\n",
    "\n",
    "    y_pred = []\n",
    "    for i in range(len(x_test)):\n",
    "        scores = []\n",
    "        for clf in classifiers:\n",
    "            score = clf.predict_proba([x_test[i]])[0][1]\n",
    "            scores.append(score)\n",
    "        y_pred.append(scores.index(max(scores)))\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy:', accuracy)\n",
    "    # return classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare your data\n",
    "# X, y = load_iris(return_X_y=True) # load the iris dataset\n",
    "\n",
    "# 2. Split your data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "train_decisionTree(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# 5. Make predictions on new examples\n",
    "# new_example = [5.1, 3.5, 1.4, 0.2] # example input features\n",
    "# scores = []\n",
    "# for clf in models:\n",
    "#     score = clf.predict_proba([new_example])[0][1]\n",
    "#     scores.append(score)\n",
    "# class_pred = scores.index(max(scores))\n",
    "# print('Predicted class:', class_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_randomForest(x_train, y_train, x_test, y_test, n_ofTrees):\n",
    "    clf = RandomForestClassifier(n_estimators=n_ofTrees, max_depth=10, random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    # Evaluate the performance of the model\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# Generate random dataset for classification\n",
    "# X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_classes=3, random_state=42)\n",
    "#read X, y from df\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = train_randomForest(X_train, y_train, X_test, y_test, 100)\n",
    "# Create a random forest classifier\n",
    "# clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "# # Train the model\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the testing data\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# # Evaluate the performance of the model\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "# print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
    "# print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
    "# print(\"F1-score:\", f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# X, y = load_digits(return_X_y=True)\n",
    "# naive_bayes = GaussianNB()\n",
    "# svc = SVC(kernel=\"rbf\", gamma=0.001)\n",
    "# The from_estimator displays the learning curve given the dataset and the predictive model to analyze. To get an estimate of the scores uncertainty, this method uses a cross-validation procedure.\n",
    "\n",
    "# from sklearn.model_selection import *\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 6), sharey=True)\n",
    "\n",
    "# common_params = {\n",
    "#     \"X\": X,\n",
    "#     \"y\": y,\n",
    "#     \"train_sizes\": np.linspace(0.1, 1.0, 5),\n",
    "#     \"cv\": ShuffleSplit(n_splits=50, test_size=0.2, random_state=0),\n",
    "#     \"score_type\": \"both\",\n",
    "#     \"n_jobs\": 4,\n",
    "#     \"line_kw\": {\"marker\": \"o\"},\n",
    "#     \"std_display_style\": \"fill_between\",\n",
    "#     \"score_name\": \"Accuracy\",\n",
    "# }\n",
    "\n",
    "# for estimator in enumerate([clf]):\n",
    "#     LearningCurveDisplay.from_estimator(estimator, **common_params, ax=ax[estimator])\n",
    "#     handles, label = ax[estimator].get_legend_handles_labels()\n",
    "#     ax[estimator].legend(handles[:2], [\"Training Score\", \"Test Score\"])\n",
    "#     ax[estimator].set_title(f\"Learning Curve for {estimator.__class__.__name__}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naiveBayes(x_train, y_train):\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selection\n",
    "# select top n features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a Pandas DataFrame\n",
    "# train_data = ['this is the first document', 'this is the second document', 'and this is the third document']\n",
    "\n",
    "# # Define the target classes\n",
    "# target_classes = ['class1', 'class2', 'class3']\n",
    "\n",
    "# #convert to data frame\n",
    "# data = pd.DataFrame({'text': train_data, 'target': target_classes})\n",
    "\n",
    "\n",
    "# Define the input features and target variable\n",
    "# X = data['text']\n",
    "# y = data['target']\n",
    "\n",
    "# Create a CountVectorizer object to convert text into numerical features\n",
    "# vectorizer = CountVectorizer()\n",
    "\n",
    "# # Convert the input features into a matrix of numerical features\n",
    "# X_matrix = vectorizer.fit_transform(X_train)\n",
    "\n",
    "model = train_naiveBayes(X_train, y_train) \n",
    "\n",
    "# Create a Multinomial Naive Bayes classifier object\n",
    "# clf = MultinomialNB()\n",
    "\n",
    "# Train the classifier on the input features and target variable\n",
    "# clf.fit(X_matrix, y)\n",
    "\n",
    "# Use the classifier to predict the class of a new document\n",
    "# new_doc = 'this is a new document'\n",
    "# new_doc_matrix = vectorizer.transform(X_test)\n",
    "# predicted_class = model.predict(X_test)\n",
    "# print(predicted_class)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Slection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_name 2.2069253075037194\n",
      "video 2.2069253075037194\n",
      "t_maximum 2.185329927575006\n",
      "r_maximum 2.143161685859663\n",
      "p_maximum 2.1418218691891604\n",
      "p_wave_range 2.094698713090448\n",
      "r_wave_range 2.0892976542007\n",
      "t_wave_range 2.0610753120784313\n",
      "p_std 1.820218031897562\n",
      "t_std 1.8132931590401324\n",
      "r_std 1.8039209691204858\n",
      "r_mean 1.6923711056451645\n",
      "p_mean 1.663547588640336\n",
      "t_mean 1.6394329266748595\n",
      "HRV_ShanEn 0.13875639405643403\n",
      "HRV_HTI 0.10314174836206735\n",
      "HRV_ApEn 0.08937845356099272\n",
      "HRV_MFDFA_alpha1_Mean 0.0842754961927712\n",
      "HRV_SD2 0.08355956196599612\n",
      "Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anwar\\AppData\\Local\\Temp\\ipykernel_19664\\4256509654.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected_features['target'] = df['target']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import matplotlib.pyplot as plt\n",
    "importance = mutual_info_classif(X,y)\n",
    "feat_importance = pd.Series(importance , X.columns[0:len(df.columns)-2])\n",
    "#sortthe features \n",
    "feat_importance = feat_importance.sort_values(ascending=False)\n",
    "\n",
    "#loop on sorted features and print \n",
    "selected_features = []\n",
    "for i in feat_importance.index:\n",
    "    if feat_importance[i] > 0.08:\n",
    "        print(i,feat_importance[i])\n",
    "        selected_features.append(i)\n",
    "\n",
    "selected_features\n",
    "\n",
    "# create a data frame of the selected features\n",
    "df_selected_features = df[selected_features]\n",
    "df_selected_features['target'] = df['target']\n",
    "df_selected_features.head()\n",
    "\n",
    "X = df_selected_features\n",
    "y = df.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = train_randomForest(X_train, y_train, X_test, y_test, 100)\n",
    "\n",
    "# feat_importance.plot(kind='barh', color = 'black')\n",
    "\n",
    "# plt.show()\n",
    "# feat_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
