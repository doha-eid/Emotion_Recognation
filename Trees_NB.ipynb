{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decisionTree(x_train, y_train, x_test, y_test, first_class_index = 0):\n",
    "    classifiers = []\n",
    "    for i in range(first_class_index, first_class_index + 9):\n",
    "        clf = DecisionTreeClassifier()\n",
    "        y_train_binary = (y_train == i)\n",
    "        clf.fit(x_train, y_train_binary)\n",
    "        classifiers.append(clf)\n",
    "\n",
    "    y_pred = []\n",
    "    for i in range(len(x_test)):\n",
    "        scores = []\n",
    "        for clf in classifiers:\n",
    "            score = clf.predict_proba([x_test[i]])[0][1]\n",
    "            scores.append(score)\n",
    "        y_pred.append(scores.index(max(scores)))\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy:', accuracy)\n",
    "    return classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39m# 2. Split your data into training and testing sets\u001b[39;00m\n\u001b[0;32m      5\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m models \u001b[39m=\u001b[39m train_decisionTree(X_train, y_train, X_test, y_test)\n\u001b[0;32m      9\u001b[0m \u001b[39m# 5. Make predictions on new examples\u001b[39;00m\n\u001b[0;32m     10\u001b[0m new_example \u001b[39m=\u001b[39m [\u001b[39m5.1\u001b[39m, \u001b[39m3.5\u001b[39m, \u001b[39m1.4\u001b[39m, \u001b[39m0.2\u001b[39m] \u001b[39m# example input features\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m, in \u001b[0;36mtrain_decisionTree\u001b[1;34m(x_train, y_train, x_test, y_test, first_class_index)\u001b[0m\n\u001b[0;32m     11\u001b[0m scores \u001b[39m=\u001b[39m []\n\u001b[0;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m clf \u001b[39min\u001b[39;00m classifiers:\n\u001b[1;32m---> 13\u001b[0m     score \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39;49mpredict_proba([x_test[i]])[\u001b[39m0\u001b[39;49m][\u001b[39m1\u001b[39;49m]\n\u001b[0;32m     14\u001b[0m     scores\u001b[39m.\u001b[39mappend(score)\n\u001b[0;32m     15\u001b[0m y_pred\u001b[39m.\u001b[39mappend(scores\u001b[39m.\u001b[39mindex(\u001b[39mmax\u001b[39m(scores)))\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "# 1. Prepare your data\n",
    "X, y = load_iris(return_X_y=True) # load the iris dataset\n",
    "\n",
    "# 2. Split your data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "models = train_decisionTree(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# 5. Make predictions on new examples\n",
    "new_example = [5.1, 3.5, 1.4, 0.2] # example input features\n",
    "scores = []\n",
    "for clf in models:\n",
    "    score = clf.predict_proba([new_example])[0][1]\n",
    "    scores.append(score)\n",
    "class_pred = scores.index(max(scores))\n",
    "print('Predicted class:', class_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_randomForest(x_train, y_train, x_test, y_test, n_ofTrees):\n",
    "    clf = RandomForestClassifier(n_estimators=n_ofTrees, max_depth=10, random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    # Evaluate the performance of the model\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# Generate random dataset for classification\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_classes=3, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_randomForest(X_train, y_train, X_test, y_test, 100)\n",
    "# Create a random forest classifier\n",
    "# clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "# # Train the model\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the testing data\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# # Evaluate the performance of the model\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "# print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
    "# print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
    "# print(\"F1-score:\", f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naiveBayes(x_train, y_train):\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m vectorizer \u001b[39m=\u001b[39m CountVectorizer()\n\u001b[0;32m     17\u001b[0m \u001b[39m# # Convert the input features into a matrix of numerical features\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m X_matrix \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(X_train)\n\u001b[0;32m     20\u001b[0m model \u001b[39m=\u001b[39m train_naiveBayes(X_matrix, y_train) \n\u001b[0;32m     22\u001b[0m \u001b[39m# Create a Multinomial Naive Bayes classifier object\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m# clf = MultinomialNB()\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39m# Use the classifier to predict the class of a new document\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m# new_doc = 'this is a new document'\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1330\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1332\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1333\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1334\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1335\u001b[0m             )\n\u001b[0;32m   1336\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1338\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1341\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1209\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1207\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1208\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1209\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1210\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1211\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    112\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:69\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[1;32m---> 69\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[0;32m     70\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Load the data into a Pandas DataFrame\n",
    "train_data = ['this is the first document', 'this is the second document', 'and this is the third document']\n",
    "\n",
    "# Define the target classes\n",
    "target_classes = ['class1', 'class2', 'class3']\n",
    "#convert to data frame\n",
    "data = pd.DataFrame({'text': train_data, 'target': target_classes})\n",
    "\n",
    "\n",
    "# Define the input features and target variable\n",
    "X = data['text']\n",
    "y = data['target']\n",
    "\n",
    "# Create a CountVectorizer object to convert text into numerical features\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# # Convert the input features into a matrix of numerical features\n",
    "X_matrix = vectorizer.fit_transform(X_train)\n",
    "\n",
    "model = train_naiveBayes(X_matrix, y_train) \n",
    "\n",
    "# Create a Multinomial Naive Bayes classifier object\n",
    "# clf = MultinomialNB()\n",
    "\n",
    "# Train the classifier on the input features and target variable\n",
    "# clf.fit(X_matrix, y)\n",
    "\n",
    "# Use the classifier to predict the class of a new document\n",
    "# new_doc = 'this is a new document'\n",
    "new_doc_matrix = vectorizer.transform(X_test)\n",
    "predicted_class = model.predict(X_test)\n",
    "print(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
